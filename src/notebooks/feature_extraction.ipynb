{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook de Feature Engineering ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données textuelles étant nettoyées, traitées et explorées, il faut maintenant que je les rende exploitables pour une future modélisation.  \n",
    "Problème, les modèles de Machine Learning ne prennent que des entrées numériques.   \n",
    "L'objectif est d'extraire une valeur mathématique du texte, plus précisement d'obtenir une représentation vectorielle de chaque document du corpus.  \n",
    "Pour cela, je vais utiliser deux familles de technique d'extraction de features:\n",
    "1. Les techniques basées sur la fréquence des mots (*Bag of Words, TF-IDF*) \n",
    "2. Les techniques basées sur la sémantique des mots (techniques d'embeddings comme *Word2Vec*, *BERT* ou *USE*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et fonctions ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Environnement de travail ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générique\n",
    "import random\n",
    "\n",
    "# Manipulation de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Project modules\n",
    "from config.paths import DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation des données ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(f'{DATA_DIR}/silver/processed_data.json').sample(10000)\n",
    "raw_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_title_tokens</th>\n",
       "      <th>processed_body_tokens</th>\n",
       "      <th>processed_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[convert, decimal, double, c]</td>\n",
       "      <td>[want, assign, decimal, variable, trans, doubl...</td>\n",
       "      <td>c#,floating-point,type-conversion,double,decimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[calculate, relative, time, c]</td>\n",
       "      <td>[given, specific, datetime, value, display, re...</td>\n",
       "      <td>c#,datetime,time,datediff,relative-time-span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[determine, user, timezone]</td>\n",
       "      <td>[standard, way, web, server, able, determine, ...</td>\n",
       "      <td>html,browser,timezone,user-agent,timezone-offset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[fastest, way, get, value, π]</td>\n",
       "      <td>[looking, fastest, way, obtain, value, π, pers...</td>\n",
       "      <td>performance,algorithm,language-agnostic,unix,pi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[use, c, socket, api, c, z, o]</td>\n",
       "      <td>[issue, getting, c, socket, api, work, properl...</td>\n",
       "      <td>c++,c,sockets,mainframe,zos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           processed_title_tokens  \\\n",
       "0   [convert, decimal, double, c]   \n",
       "1  [calculate, relative, time, c]   \n",
       "2     [determine, user, timezone]   \n",
       "3   [fastest, way, get, value, π]   \n",
       "4  [use, c, socket, api, c, z, o]   \n",
       "\n",
       "                               processed_body_tokens  \\\n",
       "0  [want, assign, decimal, variable, trans, doubl...   \n",
       "1  [given, specific, datetime, value, display, re...   \n",
       "2  [standard, way, web, server, able, determine, ...   \n",
       "3  [looking, fastest, way, obtain, value, π, pers...   \n",
       "4  [issue, getting, c, socket, api, work, properl...   \n",
       "\n",
       "                                     processed_tags  \n",
       "0  c#,floating-point,type-conversion,double,decimal  \n",
       "1      c#,datetime,time,datediff,relative-time-span  \n",
       "2  html,browser,timezone,user-agent,timezone-offset  \n",
       "3   performance,algorithm,language-agnostic,unix,pi  \n",
       "4                       c++,c,sockets,mainframe,zos  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition des fonctions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_unigram_bag_of_words(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(corpus)\n",
    "    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return bow_df\n",
    "\n",
    "def frequency_unigram_tf_idf(corpus):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "    tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    return tfidf_df\n",
    "\n",
    "def multi_label_binarizer(corpus, sep=' '):\n",
    "    corpus_list = corpus.apply(lambda x: x.split(sep))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(corpus_list)\n",
    "    tags_binarized_df = pd.DataFrame(y, columns=mlb.classes_)\n",
    "    return tags_binarized_df\n",
    "\n",
    "def display_token_info(corpus):\n",
    "    print(f'Le corpus contient {len(corpus)} tokens')\n",
    "    unique_tokens = set(corpus.split())\n",
    "    print(f\"Le corpus contient {len(unique_tokens)} tokens uniques\")\n",
    "    print(f\"Occurences moyennes par token: {len(corpus) / len(unique_tokens)}\")\n",
    "\n",
    "def inspect_non_null_matrix_values(matrix):\n",
    "    column_names = matrix.columns\n",
    "    column_name = random.choice(column_names)\n",
    "    print(\"Colonne choisie:\", column_name)\n",
    "    non_zero_column = matrix[matrix[column_name] > 0]\n",
    "    print(non_zero_column[[column_name]].head())\n",
    "\n",
    "def get_document_vector(doc, model):\n",
    "    vectors = [model.wv[token] for token in doc if token in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "def get_bert_embeddings(model, docs):\n",
    "    import torch\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    document_embeddings = []\n",
    "\n",
    "    for i, tokens in enumerate(docs):\n",
    "        print(f\"{i} embeddings générés sur {len(docs)}, {i/len(docs)*100}% effectués\")\n",
    "\n",
    "        tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            document_embeddings.append(cls_embedding)\n",
    "    \n",
    "    return document_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques basées sur la fréquence ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les techniques basées sur la fréquence représentent mathématiquement un document textuel à travers le nombre d'occurences de ces éléments, bien souvent des mots individuels, par rapport au vocabulaire entier du corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vais d'abord vectoriser mon corpus via un Bag of Words, qui va compter la fréquence d'apparation de chaque mot du vocabulaire, dans chaque document.  \n",
    "Pour ce faire, je combine d'abord les tokens de titre et de corps, pour former un seul corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_text'] = data['processed_title_tokens'].apply(lambda x: ' '.join(x)) + ' ' + data['processed_body_tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_title_tokens</th>\n",
       "      <th>processed_body_tokens</th>\n",
       "      <th>processed_tags</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[convert, decimal, double, c]</td>\n",
       "      <td>[want, assign, decimal, variable, trans, doubl...</td>\n",
       "      <td>c#,floating-point,type-conversion,double,decimal</td>\n",
       "      <td>convert decimal double c want assign decimal v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[calculate, relative, time, c]</td>\n",
       "      <td>[given, specific, datetime, value, display, re...</td>\n",
       "      <td>c#,datetime,time,datediff,relative-time-span</td>\n",
       "      <td>calculate relative time c given specific datet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[determine, user, timezone]</td>\n",
       "      <td>[standard, way, web, server, able, determine, ...</td>\n",
       "      <td>html,browser,timezone,user-agent,timezone-offset</td>\n",
       "      <td>determine user timezone standard way web serve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[fastest, way, get, value, π]</td>\n",
       "      <td>[looking, fastest, way, obtain, value, π, pers...</td>\n",
       "      <td>performance,algorithm,language-agnostic,unix,pi</td>\n",
       "      <td>fastest way get value π looking fastest way ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[use, c, socket, api, c, z, o]</td>\n",
       "      <td>[issue, getting, c, socket, api, work, properl...</td>\n",
       "      <td>c++,c,sockets,mainframe,zos</td>\n",
       "      <td>use c socket api c z o issue getting c socket ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           processed_title_tokens  \\\n",
       "0   [convert, decimal, double, c]   \n",
       "1  [calculate, relative, time, c]   \n",
       "2     [determine, user, timezone]   \n",
       "3   [fastest, way, get, value, π]   \n",
       "4  [use, c, socket, api, c, z, o]   \n",
       "\n",
       "                               processed_body_tokens  \\\n",
       "0  [want, assign, decimal, variable, trans, doubl...   \n",
       "1  [given, specific, datetime, value, display, re...   \n",
       "2  [standard, way, web, server, able, determine, ...   \n",
       "3  [looking, fastest, way, obtain, value, π, pers...   \n",
       "4  [issue, getting, c, socket, api, work, properl...   \n",
       "\n",
       "                                     processed_tags  \\\n",
       "0  c#,floating-point,type-conversion,double,decimal   \n",
       "1      c#,datetime,time,datediff,relative-time-span   \n",
       "2  html,browser,timezone,user-agent,timezone-offset   \n",
       "3   performance,algorithm,language-agnostic,unix,pi   \n",
       "4                       c++,c,sockets,mainframe,zos   \n",
       "\n",
       "                                       combined_text  \n",
       "0  convert decimal double c want assign decimal v...  \n",
       "1  calculate relative time c given specific datet...  \n",
       "2  determine user timezone standard way web serve...  \n",
       "3  fastest way get value π looking fastest way ob...  \n",
       "4  use c socket api c z o issue getting c socket ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysons ce nouveau corpus combiné:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le corpus contient 37110150 tokens\n",
      "Le corpus contient 196190 tokens uniques\n",
      "Occurences moyennes par token: 189.15413629644732\n"
     ]
    }
   ],
   "source": [
    "display_token_info(' '.join(data.combined_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je peux donc passer au Bag of Words, où je m'attends à une matrice contenant des vecteurs d'environ 196000 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = frequency_unigram_bag_of_words(data.combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>0000000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>000000000</th>\n",
       "      <th>000000000000</th>\n",
       "      <th>0000000000000000</th>\n",
       "      <th>...</th>\n",
       "      <th>香川県</th>\n",
       "      <th>高知県</th>\n",
       "      <th>鳥取県</th>\n",
       "      <th>鹿児島県</th>\n",
       "      <th>麗安</th>\n",
       "      <th>龥а</th>\n",
       "      <th>한글</th>\n",
       "      <th>ｻｿ</th>\n",
       "      <th>ﾎｱﾎｻﾎｷﾎｼﾎｭﾏ</th>\n",
       "      <th>ﾎｺﾏ狐πｼﾎｵ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 196011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000  000000  0000000  00000000  000000000  000000000000  \\\n",
       "0   0    0     0      0       0        0         0          0             0   \n",
       "1   0    0     0      0       0        0         0          0             0   \n",
       "2   0    0     0      0       0        0         0          0             0   \n",
       "3   0    0     0      0       0        0         0          0             0   \n",
       "4   0    0     0      0       0        0         0          0             0   \n",
       "\n",
       "   0000000000000000  ...  香川県  高知県  鳥取県  鹿児島県  麗安  龥а  한글  ｻｿ  ﾎｱﾎｻﾎｷﾎｼﾎｭﾏ  \\\n",
       "0                 0  ...    0    0    0     0   0   0   0   0            0   \n",
       "1                 0  ...    0    0    0     0   0   0   0   0            0   \n",
       "2                 0  ...    0    0    0     0   0   0   0   0            0   \n",
       "3                 0  ...    0    0    0     0   0   0   0   0            0   \n",
       "4                 0  ...    0    0    0     0   0   0   0   0            0   \n",
       "\n",
       "   ﾎｺﾏ狐πｼﾎｵ  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 196011 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne choisie: nuthin\n",
      "      nuthin\n",
      "6414       1\n"
     ]
    }
   ],
   "source": [
    "inspect_non_null_matrix_values(bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vecteurs ont une très forte dimensionalité, et sont majoritairement creux (beaucoup de 0, peu de 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour prendre en compte la rareté relative d'un mot et pondérer sa fréquence par son importance dans le document, TF-IDF peut être utilisé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = frequency_unigram_tf_idf(data.combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>0000000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>000000000</th>\n",
       "      <th>000000000000</th>\n",
       "      <th>0000000000000000</th>\n",
       "      <th>...</th>\n",
       "      <th>香川県</th>\n",
       "      <th>高知県</th>\n",
       "      <th>鳥取県</th>\n",
       "      <th>鹿児島県</th>\n",
       "      <th>麗安</th>\n",
       "      <th>龥а</th>\n",
       "      <th>한글</th>\n",
       "      <th>ｻｿ</th>\n",
       "      <th>ﾎｱﾎｻﾎｷﾎｼﾎｭﾏ</th>\n",
       "      <th>ﾎｺﾏ狐πｼﾎｵ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41760</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31954</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21289</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23830</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 196011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  0000  00000  000000  0000000  00000000  000000000  \\\n",
       "41760  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0   \n",
       "29645  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0   \n",
       "31954  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0   \n",
       "21289  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0   \n",
       "23830  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0   \n",
       "\n",
       "       000000000000  0000000000000000  ...  香川県  高知県  鳥取県  鹿児島県   麗安   龥а  \\\n",
       "41760           0.0               0.0  ...  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "29645           0.0               0.0  ...  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "31954           0.0               0.0  ...  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "21289           0.0               0.0  ...  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "23830           0.0               0.0  ...  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "\n",
       "        한글   ｻｿ  ﾎｱﾎｻﾎｷﾎｼﾎｭﾏ  ﾎｺﾏ狐πｼﾎｵ  \n",
       "41760  0.0  0.0          0.0       0.0  \n",
       "29645  0.0  0.0          0.0       0.0  \n",
       "31954  0.0  0.0          0.0       0.0  \n",
       "21289  0.0  0.0          0.0       0.0  \n",
       "23830  0.0  0.0          0.0       0.0  \n",
       "\n",
       "[5 rows x 196011 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne choisie: advancedautocomplete\n",
      "       advancedautocomplete\n",
      "12368              0.153954\n"
     ]
    }
   ],
   "source": [
    "inspect_non_null_matrix_values(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malgré la pondération, la matrice du TF-IDF garde les mêmes caractéristiques, et inconvénients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-label Binarizer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les tags, une approche fréquemment utilisée est de transformer la liste de labels en vecteurs binaires; 1 si le tag est présent dans le document, 0 si non.  \n",
    "L'approche est semblable au Bag of Words, mais sera très utile lors de l'étape de modélisation et de classification supervisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_matrix = multi_label_binarizer(data.processed_tags, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16-bit</th>\n",
       "      <th>24-bit</th>\n",
       "      <th>256color</th>\n",
       "      <th>2d</th>\n",
       "      <th>2d-3d-conversion</th>\n",
       "      <th>2d-games</th>\n",
       "      <th>2phase-commit</th>\n",
       "      <th>3-tier</th>\n",
       "      <th>3-way-merge</th>\n",
       "      <th>32-bit</th>\n",
       "      <th>...</th>\n",
       "      <th>zope.interface</th>\n",
       "      <th>zorba</th>\n",
       "      <th>zos</th>\n",
       "      <th>zpl</th>\n",
       "      <th>zpl-ii</th>\n",
       "      <th>zpt</th>\n",
       "      <th>zsh</th>\n",
       "      <th>zsh-completion</th>\n",
       "      <th>zsi</th>\n",
       "      <th>zsync</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13192</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37602</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13347 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       16-bit  24-bit  256color  2d  2d-3d-conversion  2d-games  \\\n",
       "19987       0       0         0   0                 0         0   \n",
       "13192       0       0         0   0                 0         0   \n",
       "7988        0       0         0   0                 0         0   \n",
       "20798       0       0         1   0                 0         0   \n",
       "37602       0       0         0   0                 0         0   \n",
       "\n",
       "       2phase-commit  3-tier  3-way-merge  32-bit  ...  zope.interface  zorba  \\\n",
       "19987              0       0            0       0  ...               0      0   \n",
       "13192              0       0            0       0  ...               0      0   \n",
       "7988               0       0            0       0  ...               0      0   \n",
       "20798              0       0            0       0  ...               0      0   \n",
       "37602              0       0            0       0  ...               0      0   \n",
       "\n",
       "       zos  zpl  zpl-ii  zpt  zsh  zsh-completion  zsi  zsync  \n",
       "19987    0    0       0    0    0               0    0      0  \n",
       "13192    0    0       0    0    0               0    0      0  \n",
       "7988     0    0       0    0    0               0    0      0  \n",
       "20798    0    0       0    0    0               0    0      0  \n",
       "37602    0    0       0    0    0               0    0      0  \n",
       "\n",
       "[5 rows x 13347 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb_matrix.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne choisie: cdo.message\n",
      "       cdo.message\n",
      "1609             1\n",
      "40833            1\n"
     ]
    }
   ],
   "source": [
    "inspect_non_null_matrix_values(mlb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2684"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb_matrix['python'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques basées sur la sémantique ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les techniques basées sur la sémantique font la promesse de représenter les mots selon leur contexte, en rapprochant mathématiquement les mots sémantiquement proches, tout en gardant une dimensionnalité mesurées par rapport aux techniques fréquentistes. Les vecteurs résultants de ces techniques sont appelés *embeddings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec est une des premières techniques d'embedding qui a vu le jour.  \n",
    "Il génère des embeddings en capturant les contextes des mots présents, et en rapprochant les mots aux contextes ressemblants.  \n",
    "Après avoir formaté le corpus en fonction des besoins du modèle, je l'entraîne pour obtenir mes embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['combined_text'].apply(lambda x: simple_preprocess(x)).tolist()\n",
    "model_w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.4548879e-01, -5.4499847e-01,  3.5737474e+00,  7.4911016e-01,\n",
       "        1.7693719e+00,  9.3174100e-01, -1.8137324e+00,  7.2189242e-01,\n",
       "        1.0077080e+00, -9.2219323e-01,  5.7912534e-01, -1.9168587e+00,\n",
       "       -1.3907452e+00, -6.8253934e-01,  2.1553979e+00, -4.8772052e-01,\n",
       "        8.9186102e-01, -1.3156390e+00, -2.0365989e-01,  3.8702691e-01,\n",
       "        2.2898646e-01,  3.7270589e+00,  2.6033237e+00,  2.6249492e+00,\n",
       "        6.9601542e-01, -3.1900844e-01,  2.1758277e+00,  4.2216396e-01,\n",
       "        1.5308858e+00,  2.0585020e+00,  2.6807017e+00, -2.5929253e+00,\n",
       "        3.4810323e-01,  2.6474607e+00, -3.3339436e+00, -1.4467429e-01,\n",
       "        1.1285890e+00,  5.1117235e-01, -8.5784709e-01,  2.7665594e-01,\n",
       "        3.7365857e-01, -1.5771922e-01,  1.0731281e+00, -1.5863495e+00,\n",
       "       -4.9431810e-01,  5.2018380e-01,  3.3274967e-02, -2.1991923e+00,\n",
       "       -9.1936046e-01, -2.2869666e+00, -5.2085418e-01, -1.5742459e+00,\n",
       "       -1.0868225e+00, -2.2610958e+00,  5.0426376e-01, -2.0774324e+00,\n",
       "       -1.7712927e+00,  7.0044023e-01,  1.4508504e-01, -3.3840722e-01,\n",
       "        5.0806904e-01, -2.4479362e-01,  4.9265581e-01, -1.8921649e+00,\n",
       "        5.0989079e-01,  1.8924479e+00, -5.7736975e-01,  4.4447795e-01,\n",
       "       -3.6692464e-01, -2.4273458e+00, -6.8587005e-01, -2.0873172e+00,\n",
       "        1.4888345e-01, -1.1841277e+00,  3.3793218e+00, -1.9203780e+00,\n",
       "       -7.9162061e-01, -1.9530897e+00, -8.4730792e-01, -5.0221930e+00,\n",
       "       -4.9780305e-03, -1.4450910e+00, -1.2456379e+00, -9.4860566e-01,\n",
       "        9.7090930e-01,  4.9257600e-01, -2.0139904e+00, -2.2374272e-01,\n",
       "       -6.2565297e-01, -3.5057034e+00, -3.3810217e+00, -4.0429595e-01,\n",
       "        6.0725278e-01, -3.1618164e+00,  2.6101384e+00,  1.8263534e-01,\n",
       "       -2.0399500e-01, -2.9183987e-01, -4.0817600e-01,  2.3149488e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv['python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec génère des embeddings individuels; j'utilise donc une technique d'agrégation pour obtenir un embedding par document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = [get_document_vector(doc, model_w2v) for doc in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.2622167e-01,  2.7647746e-01,  5.1938003e-01,  1.2976211e-01,\n",
       "       -5.2795641e-02, -1.1486957e+00, -5.9895384e-01,  1.2291791e+00,\n",
       "        7.8579724e-01, -3.9071169e-01, -4.9083763e-01, -5.4303080e-01,\n",
       "        6.2502436e-02, -4.6656066e-01,  9.9519950e-01, -1.3299716e-01,\n",
       "       -5.2796316e-01, -7.3648864e-01,  9.2447102e-02, -3.0692586e-01,\n",
       "        3.2516250e-01,  1.4903946e-01,  2.1990379e-02, -4.7518584e-01,\n",
       "        4.9597815e-01,  4.1939402e-01, -8.4135807e-01,  2.4281338e-01,\n",
       "       -5.0183588e-01,  3.8527343e-01,  9.2106491e-01,  1.2163569e-01,\n",
       "       -3.5071781e-01,  6.7121655e-01, -7.0472580e-01, -1.3362548e-01,\n",
       "        6.2825400e-01, -6.4591366e-01, -8.1976706e-01,  7.8207225e-01,\n",
       "        4.5443037e-01,  9.1509473e-01, -1.7083572e-01, -6.5209574e-01,\n",
       "       -6.0167901e-02, -6.3294250e-01,  2.6322955e-01, -5.6927526e-01,\n",
       "        2.4256280e-02,  2.5795752e-01,  7.3007032e-02, -5.7895046e-01,\n",
       "       -1.0459632e+00,  5.1859361e-01, -4.9308769e-02,  1.9598074e-01,\n",
       "       -6.2645102e-01, -7.1109384e-01, -5.0935107e-01,  2.4445207e-01,\n",
       "        6.3967389e-01,  8.3152729e-01,  1.2160263e-03, -8.1583530e-01,\n",
       "        6.8814689e-03,  1.4039668e-01, -7.0090061e-01,  2.0750524e-01,\n",
       "       -5.4855424e-01, -3.2135010e-02, -6.6981882e-01,  1.0616887e+00,\n",
       "       -1.3633437e-01, -7.0175362e-01,  7.2414732e-01,  1.0428391e+00,\n",
       "       -1.6928676e-01,  2.6129633e-01, -7.2358894e-01,  1.7244928e-01,\n",
       "        3.4604147e-01,  1.7571155e-02, -3.0274028e-01, -2.7335998e-01,\n",
       "        1.4555253e-01, -6.6463417e-01, -1.0070283e+00,  6.1772412e-01,\n",
       "       -4.9899006e-01, -1.8470687e+00,  4.6609515e-01,  5.0373513e-01,\n",
       "       -6.4605075e-01, -8.3873600e-01,  4.4196206e-01, -1.4284849e-02,\n",
       "        9.0408605e-01,  1.7070313e-01, -5.7636023e-01, -8.3459795e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{DATA_DIR}/gold/embeddings_word2vec.npy', document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'obtiens bien les embeddings de chaque document ici.  \n",
    "L'inconvénient principal de Word2Vec se situe dans son appréciation limitée du contexte des mots; un même mot aux multiples sens n'aura qu'un embedding, au lieu de un par sens différent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT est un embedder pré-entraîné, basé sur l'architecture Transformer, ce qui le rend très puissant pour comprendre le contexte d'un mot en profondeur.  \n",
    "Je vais donc obtenir des embeddings contextuels, basés sur les données qui ont servies à son entraînement, et adaptés au document fourni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaspardhurez/csprojects/repos/openclassrooms/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertModel.from_pretrained('albert-base-v2')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data['combined_text'].tolist()\n",
    "tokenized_docs = [tokenizer(doc, return_tensors='pt', truncation=True, padding=True) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings = get_bert_embeddings(model, tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{DATA_DIR}/gold/embeddings_bert.npy', bert_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE (Universal Sentence Encoder) est un autre embedder pré-entraîné, développé par Google.  \n",
    "Il est plus efficace et moins gourmand en ressources que BERT, au compromis d'un contexte moins granulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to load a model of incompatible/unknown type. '/var/folders/r2/ss_qsp851qbd7s57ljmw44480000gn/T/tfhub_modules/063d866c06683311b44b4992fd46003be952409c' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/universal-sentence-encoder/4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/csprojects/repos/openclassrooms/.venv/lib/python3.11/site-packages/tensorflow_hub/module_v2.py:113\u001b[0m, in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    108\u001b[0m saved_model_pbtxt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    109\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(module_path),\n\u001b[1;32m    110\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_path) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_pbtxt_path)):\n\u001b[0;32m--> 113\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load a model of incompatible/unknown type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains neither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m nor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    115\u001b[0m                    (module_path, tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB,\n\u001b[1;32m    116\u001b[0m                     tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options:\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(tf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoadOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to load a model of incompatible/unknown type. '/var/folders/r2/ss_qsp851qbd7s57ljmw44480000gn/T/tfhub_modules/063d866c06683311b44b4992fd46003be952409c' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'."
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_embeddings = embed(documents).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 512)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04490682, -0.06579038, -0.00030356,  0.06503192,  0.05765333,\n",
       "       -0.00393299, -0.02149728, -0.06094711, -0.03997018,  0.06284467,\n",
       "       -0.00920186, -0.02412126, -0.05465421,  0.06364406,  0.00042385,\n",
       "        0.06579082, -0.05108438,  0.00583293,  0.03009957, -0.06381157,\n",
       "       -0.03322913, -0.05842859,  0.04841243, -0.01356049, -0.03274378,\n",
       "        0.02555372, -0.03763048, -0.03119965, -0.05024819, -0.02886915,\n",
       "       -0.00857334, -0.03321578, -0.03993635, -0.06399625, -0.06562254,\n",
       "        0.03072463, -0.05723202, -0.05265672,  0.02365536,  0.04956209,\n",
       "        0.03708434, -0.01723283,  0.04148487,  0.04844912,  0.06577364,\n",
       "       -0.04618986, -0.06129446,  0.02435905, -0.05565225,  0.06529625,\n",
       "       -0.06421264,  0.00464133, -0.04064857, -0.03056659,  0.03985272,\n",
       "       -0.04536984, -0.05194784,  0.02306649, -0.05845733, -0.03359056,\n",
       "       -0.04997613,  0.04903685,  0.05373524, -0.02376573, -0.06419381,\n",
       "       -0.0608025 , -0.04520904,  0.0475619 ,  0.03352387,  0.00087311,\n",
       "       -0.06569377,  0.03940976,  0.03714414,  0.04358046, -0.02393971,\n",
       "        0.00680921, -0.04624926,  0.03083343, -0.03530322, -0.00484309,\n",
       "       -0.05462677,  0.06025665, -0.00117808,  0.0396093 ,  0.03501837,\n",
       "        0.01313197, -0.01997936,  0.00255672, -0.0598243 , -0.02387818,\n",
       "       -0.00537257,  0.01768786,  0.06169503, -0.04354811, -0.01914683,\n",
       "        0.05033089, -0.01340729, -0.05134119,  0.0471761 , -0.06579044,\n",
       "        0.00035674,  0.05631263, -0.06278306, -0.01428661, -0.01153362,\n",
       "       -0.0483187 , -0.05363673,  0.05488454,  0.03406113,  0.04183341,\n",
       "       -0.03801994, -0.06393821,  0.05407426, -0.02905237, -0.00071674,\n",
       "        0.04076112,  0.0493397 , -0.06125853,  0.05810328, -0.02897091,\n",
       "        0.03203601,  0.04995066, -0.06550766,  0.04939918,  0.02958216,\n",
       "        0.0215605 , -0.02152193, -0.04078116,  0.04972984, -0.064465  ,\n",
       "        0.05734545, -0.01527277, -0.0211484 , -0.06190894, -0.00774186,\n",
       "        0.01919006, -0.05858853, -0.05098842, -0.03987226,  0.05108205,\n",
       "       -0.00620283,  0.06510019,  0.03110554,  0.06539684,  0.02838685,\n",
       "       -0.06556283,  0.00095693, -0.05929412, -0.0022823 , -0.00989217,\n",
       "        0.02054036,  0.06577297,  0.05696034, -0.06535698, -0.04634486,\n",
       "        0.05517126,  0.06479553, -0.04777259,  0.0055395 , -0.00328445,\n",
       "       -0.06251393,  0.05491182,  0.00171062,  0.06399307, -0.05988544,\n",
       "       -0.01171957, -0.04367494,  0.0187844 ,  0.00536674, -0.04456799,\n",
       "        0.01465234, -0.0643679 ,  0.04910584,  0.00993518, -0.03721695,\n",
       "       -0.05029002,  0.01592884, -0.06294262,  0.01365636,  0.0059241 ,\n",
       "       -0.03280593,  0.04759092, -0.0251356 ,  0.03256157, -0.05430496,\n",
       "        0.0560341 ,  0.03304661,  0.065786  ,  0.03331956,  0.0528057 ,\n",
       "        0.06362424,  0.04845152, -0.05695944,  0.01750855,  0.05307847,\n",
       "       -0.0145793 ,  0.04904274,  0.06579074,  0.03229794,  0.04229549,\n",
       "        0.01201766,  0.01436563,  0.06189505,  0.05237115, -0.04290709,\n",
       "       -0.06277894, -0.03886776, -0.06507508, -0.06577776,  0.04764751,\n",
       "        0.01915361, -0.00594491, -0.00510557, -0.01673162, -0.05880663,\n",
       "        0.02506791,  0.01199106,  0.01951866, -0.00988868, -0.047945  ,\n",
       "       -0.03494924, -0.00785503, -0.04524616,  0.01117162, -0.04625505,\n",
       "        0.04110017,  0.01200229,  0.00188792, -0.01327945,  0.00411585,\n",
       "        0.01233421, -0.05552441,  0.06249306, -0.06567903,  0.0647034 ,\n",
       "        0.04125282,  0.06577686,  0.03656954, -0.0256547 ,  0.05663589,\n",
       "        0.05859104,  0.03681907, -0.06489773,  0.06133067, -0.01401691,\n",
       "        0.05723706, -0.00228345, -0.05595469,  0.0626789 ,  0.06381045,\n",
       "        0.02681805,  0.03278926,  0.04314446,  0.04455236,  0.02691816,\n",
       "       -0.02861033, -0.0572409 ,  0.06389393, -0.03287338, -0.0266747 ,\n",
       "       -0.06336482,  0.00785183, -0.06059468,  0.0462539 ,  0.05846648,\n",
       "        0.04190538, -0.05385435,  0.0254668 , -0.0065157 , -0.03292852,\n",
       "       -0.04415961,  0.00713488, -0.06539639, -0.05995136, -0.04844764,\n",
       "        0.03737721, -0.02604929,  0.0022136 ,  0.06578965, -0.0491909 ,\n",
       "        0.05891865, -0.04978233, -0.04560216,  0.06208916, -0.01798253,\n",
       "       -0.0649849 ,  0.02732002,  0.06309698, -0.03818611,  0.00216642,\n",
       "        0.05920379,  0.06270992, -0.00782478,  0.06383955,  0.06046066,\n",
       "        0.00516331, -0.05910742, -0.00269693,  0.0180887 , -0.00533685,\n",
       "        0.0511476 , -0.05899177, -0.05676861,  0.06449034, -0.00409484,\n",
       "       -0.05166779, -0.05617692, -0.0570452 ,  0.0419127 ,  0.04310707,\n",
       "       -0.05775658,  0.00523441, -0.01040954, -0.02911296, -0.00751634,\n",
       "        0.03111359, -0.03622489, -0.01186375,  0.01604666,  0.01457266,\n",
       "       -0.05300386, -0.03874589,  0.0144438 , -0.03096354, -0.05401681,\n",
       "        0.03437576, -0.06392708,  0.02885868, -0.06579044,  0.0214458 ,\n",
       "        0.05541139, -0.04795881,  0.01821761,  0.0651457 ,  0.00424258,\n",
       "       -0.04289456, -0.06472605, -0.01617369,  0.06318674,  0.0387402 ,\n",
       "       -0.03724371, -0.04077255, -0.05013622,  0.01776562, -0.06579087,\n",
       "       -0.03954059, -0.03938329, -0.06410706, -0.02697817,  0.02065306,\n",
       "       -0.00282369,  0.02979242,  0.00230237,  0.06429654, -0.02103613,\n",
       "        0.02903417, -0.06578595, -0.00279946,  0.0054564 , -0.03760056,\n",
       "       -0.04963651,  0.00187113, -0.06578974,  0.04400816,  0.05623408,\n",
       "       -0.04853082,  0.03549047, -0.03758034,  0.0657734 ,  0.05180418,\n",
       "       -0.01775529, -0.0156056 ,  0.03991233, -0.06166538,  0.05486726,\n",
       "       -0.03647069,  0.04125925, -0.04952464,  0.04990058, -0.05167682,\n",
       "       -0.0286135 , -0.02187483, -0.04003855, -0.06264853,  0.01054516,\n",
       "       -0.06356309,  0.05396204,  0.06147148,  0.06432112,  0.06288452,\n",
       "       -0.05784075, -0.06303403,  0.0414321 , -0.00821863,  0.04583317,\n",
       "        0.06133117, -0.01451829,  0.04196565,  0.04710209,  0.06000406,\n",
       "        0.04092646, -0.05962989, -0.02020944,  0.03982974,  0.05526454,\n",
       "        0.02492104, -0.06556402,  0.04107297,  0.05376951,  0.02957053,\n",
       "        0.01584962, -0.04662184, -0.02201587, -0.04289399, -0.05509506,\n",
       "        0.00644017, -0.06425425,  0.04921799, -0.05608184, -0.00177585,\n",
       "       -0.03289436,  0.00045734,  0.0129803 , -0.06196935, -0.04158327,\n",
       "       -0.03140914, -0.01585948,  0.06137146,  0.05114164,  0.02868721,\n",
       "        0.04915736,  0.05865387, -0.0499434 , -0.03786342, -0.05593995,\n",
       "        0.0218623 ,  0.01814045, -0.03592407, -0.02641517,  0.03341956,\n",
       "        0.04314791,  0.0129675 , -0.02912956,  0.01324363, -0.02496479,\n",
       "        0.05038524, -0.04488355, -0.02141477, -0.04728934,  0.0655741 ,\n",
       "       -0.0403409 ,  0.02689883,  0.06575925,  0.06148374, -0.04606424,\n",
       "       -0.06370688, -0.06144084,  0.0455286 ,  0.06421755, -0.03721695,\n",
       "        0.04831211, -0.00400274, -0.03738119,  0.0200402 ,  0.06215879,\n",
       "       -0.04838414, -0.02910721,  0.04510161,  0.03265549,  0.06345862,\n",
       "       -0.06141507, -0.01903104, -0.05948587,  0.0529665 ,  0.06520058,\n",
       "        0.05402967, -0.04518384, -0.05622172, -0.03385077, -0.06577678,\n",
       "        0.05970762,  0.03466409,  0.03240417,  0.02397836, -0.00574628,\n",
       "        0.06090793,  0.04194352, -0.05866092,  0.05274953,  0.01479503,\n",
       "        0.06557819,  0.03191438,  0.02692811, -0.05352019,  0.0468107 ,\n",
       "       -0.06570986,  0.06156008,  0.01422861,  0.0289372 , -0.04748103,\n",
       "       -0.01972305,  0.02307565,  0.05925035, -0.06309896,  0.05167069,\n",
       "       -0.01849472,  0.04754774, -0.02588872, -0.05785442,  0.0276085 ,\n",
       "        0.03281619, -0.03630655], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{DATA_DIR}/gold/embeddings_use.npy', use_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai généré tous mes embeddings; je peux maintenant exporter la matrice de features complète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(f'{DATA_DIR}/gold/feature_matrix.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
